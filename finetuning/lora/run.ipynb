{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65082c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "t.manual_seed(0)\n",
    "\n",
    "DATA_PATH=\"../../datasets\"\n",
    "DATASET_NAME=\"dune\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-base\"\n",
    "\n",
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.constants import HF_HUB_CACHE\n",
    "\n",
    "HF_HUB_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bca340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen3ForCausalLM, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch, tokenizer: AutoTokenizer):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=False, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50982ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(DATA_PATH, \"processed\", DATASET_NAME)\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    ds = load_from_disk(dataset_path)\n",
    "else:\n",
    "    text = open(os.path.join(DATA_PATH, \"dune.txt\")).read()\n",
    "    chunk_size = 1024\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size) if text[i:i+chunk_size].strip()]\n",
    "\n",
    "    raw = Dataset.from_list([{\"text\": c} for c in chunks]).train_test_split(test_size=0.1)\n",
    "\n",
    "    ds = raw.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=raw[\"train\"].column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "\n",
    "    os.makedirs(dataset_path)\n",
    "    ds.save_to_disk(dataset_path)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: Qwen3ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=t.bfloat16,  # instead of dtype=\"auto\"\n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "assert model.device.type == DEVICE\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ff74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "prompt = \"Paul\"\n",
    "\n",
    "def generate(model, prompt):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # conduct text completion\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def generate_stream(model, prompt, max_new_tokens=150):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with t.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        input_ids = t.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "generate_stream(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total num params: {model.num_parameters(True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27398c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0afb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import apply_lora, enable_lora\n",
    "\n",
    "@dataclass\n",
    "class LoraArguments:\n",
    "    batch_size=2\n",
    "    rank=8\n",
    "    alpha=16.0\n",
    "\n",
    "\n",
    "apply_lora(model, target_modules=(\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"), rank=LoraArguments.rank, alpha=LoraArguments.alpha)\n",
    "num_lora_params = enable_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num params (original): {model.num_parameters(False) - num_lora_params}\")\n",
    "print(f\"num params (after lora): {model.num_parameters(False)}\")\n",
    "\n",
    "print(f\"num params added by lora: {num_lora_params}\")\n",
    "print(f\"lora params %: {num_lora_params / model.num_parameters(False) * 100.}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, trainset: Dataset, epochs=1):\n",
    "    model.train()\n",
    "    model.config.use_cache = False\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=LoraArguments.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = t.optim.AdamW(trainable, lr=2e-4, weight_decay=0.0)\n",
    "    loss_list = []\n",
    "    ema = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm.tqdm(trainloader)\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits  # [B,T,V]\n",
    "\n",
    "            logits = logits[:, :-1, :]\n",
    "            targets = input_ids[:, 1:]\n",
    "            mask = attention_mask[:, 1:].bool()\n",
    "            targets = targets.masked_fill(~mask, -100)\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1),\n",
    "                ignore_index=-100,\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            t.nn.utils.clip_grad_norm_(trainable, 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            li = loss.item()\n",
    "            loss_list.append(li)\n",
    "            ema = li if ema is None else 0.98 * ema + 0.02 * li\n",
    "            pbar.set_postfix(epoch=f\"{epoch+1}/{epochs}\", loss=f\"{li:.3f}\", ema=f\"{ema:.3f}\")\n",
    "\n",
    "    return loss_list\n",
    "\n",
    "            \n",
    "train(model, ds[\"train\"], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stream(model, \"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
