{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65082c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "t.manual_seed(0)\n",
    "\n",
    "DATA_PATH=\"../../datasets\"\n",
    "DATASET_NAME=\"dune\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-base\"\n",
    "\n",
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.constants import HF_HUB_CACHE\n",
    "\n",
    "HF_HUB_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bca340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen3ForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch, tokenizer: AutoTokenizer):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50982ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(DATA_PATH, \"processed\", DATASET_NAME)\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    ds = load_from_disk(dataset_path)\n",
    "else:\n",
    "    text = open(os.path.join(DATA_PATH, \"dune.txt\")).read()\n",
    "    chunk_size = 1024\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size) if text[i:i+chunk_size].strip()]\n",
    "\n",
    "    raw = Dataset.from_list([{\"text\": c} for c in chunks]).train_test_split(test_size=0.1)\n",
    "\n",
    "    ds = raw.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=raw[\"train\"].column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "\n",
    "    os.makedirs(dataset_path)\n",
    "    ds.save_to_disk(dataset_path)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: Qwen3ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",\n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "assert model.device.type == DEVICE\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ff74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "prompt = \"Paul\"\n",
    "\n",
    "def generate(model, prompt):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # conduct text completion\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=150\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def generate_stream(model, prompt, max_new_tokens=150):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with t.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "\n",
    "        next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "        input_ids = t.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "generate_stream(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total num params: {model.num_parameters(True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParameterization(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=1, alpha=1., device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.lora_A = nn.Parameter(t.randn((rank, self.out_features)).to(device))\n",
    "        self.lora_B = nn.Parameter(t.zeros((self.in_features, rank)).to(device))\n",
    "\n",
    "        self.scale = alpha/rank\n",
    "        self.enabled = False\n",
    "    \n",
    "    def forward(self, w: t.Tensor):\n",
    "        if self.enabled:\n",
    "            assert w.shape == (self.out_features, self.in_features)\n",
    "            return w + (self.lora_B @ self.lora_A) * self.scale\n",
    "        return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parametrize\n",
    "\n",
    "def apply_lora(model: nn.Module, target_modules=(\"q_proj\"), rank=8, alpha=16):    \n",
    "    for name, module in model.named_modules():\n",
    "        if not isinstance(module, nn.Linear):\n",
    "            continue\n",
    "        \n",
    "        if not any(m in name for m in target_modules):\n",
    "            continue\n",
    "        \n",
    "        parametrize.register_parametrization(\n",
    "            module,\n",
    "            \"weight\",\n",
    "            LoRAParameterization(\n",
    "                in_features=module.in_features,\n",
    "                out_features=module.out_features,\n",
    "                rank=rank,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "        )\n",
    "\n",
    "def enable_lora(model: nn.Module):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    for m in model.modules():\n",
    "        if not parametrize.is_parametrized(m, \"weight\"):\n",
    "            continue\n",
    "                \n",
    "        m.parametrizations.weight[0].enabled = True\n",
    "        for p in m.parametrizations.weight[0].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    added_params = model.num_parameters(True)\n",
    "    \n",
    "    return added_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0afb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoraArguments:\n",
    "    batch_size=8,\n",
    "    rank=8,\n",
    "    alpha=1.0,    \n",
    "\n",
    "apply_lora(model, target_modules=(\"q_proj\", \"k_proj\", \"v_proj\"), rank=LoraArguments.rank, alpha=LoraArguments.alpha)\n",
    "num_lora_params = enable_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num params (original): {model.num_parameters(False) - num_lora_params}\")\n",
    "print(f\"num params (after lora): {model.num_parameters(False)}\")\n",
    "\n",
    "print(f\"num params added by lora: {num_lora_params}\")\n",
    "print(f\"lora params %: {num_lora_params / model.num_parameters(False) * 100.}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, col_names=[\"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, trainset: DataLoader, epochs=1):\n",
    "    trainloader = DataLoader(trainset, batch_size=LoraArguments.batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm.tqdm(trainloader)\n",
    "\n",
    "        for x, y in pbar:\n",
    "            # Move data to device, perform forward pass\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "\n",
    "            # Calculate loss, perform backward pass\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update logs & progress bar\n",
    "            loss_list.append(loss.item())\n",
    "            pbar.set_postfix(epoch=f\"{epoch + 1}/{epochs}\", loss=f\"{loss:.3f}\")\n",
    "        \n",
    "train(model, ds[\"train\"], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ac3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stream(model, prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
